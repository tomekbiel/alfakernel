<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Beverage Quality Classification with Random Forest</title>
    <link rel="stylesheet" href="../../../css/modern-style.css">
    <link href="https://fonts.googleapis.com/css2?family=MonteCarlo&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <!-- Code highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <link rel="stylesheet" href="../../../css/code-highlight.css">
</head>
<body class="projects-bg">
    <div class="container">
        <!-- Header -->
        <header class="header">
            <div class="header-left initials">
                <span>TB</span>
            </div>
            <div class="header-center">
                <a href="/alfakernel/index.html" class="logo">AlfaKernel</a>
                <div class="subtitle">The Core of Quantitative Finance</div>
            </div>
            <div class="header-right login-container">
                <button id="login-btn" class="login-btn">
                    <i class="fas fa-lock"></i> Sign in
                </button>
                <div id="login-form" class="login-form hidden">
                    <input type="email" placeholder="Email" />
                    <input type="password" placeholder="Password" />
                    <button type="submit" class="login-btn">Log In</button>
                </div>
            </div>
        </header>

        <div class="flex-layout">
            <!-- Sidebar -->
            <aside class="sidebar">
                <div class="tab" data-tab="overview">Overview</div>
                <div class="tab" data-tab="analysis">Analysis</div>
                <div class="tab" data-tab="report">Technical Report</div>
            </aside>

            <!-- Main Content -->
            <main class="main-content">
                <!-- Overview -->
                <section id="overview" class="tab-content active">
                    <h2>Beverage Quality Prediction with Random Forest</h2>
                    <div class="project-overview">
                        <p>This project demonstrates a machine learning approach to predict beverage quality using Random Forest Classifier and other techniques.</p>
                        <div class="quick-links" style="margin-bottom: 1.5em;">
                            <a href="https://github.com/tomekbiel/python_data_analysis/blob/random_forest_example/random_forest_baverages.ipynb">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                            <a href="https://colab.research.google.com/github/tomekbiel/python_data_analysis/blob/random_forest_example/random_forest_baverages.ipynb" target="_blank" class="button">
                                <i class="fab fa-google"></i> Colab
                            </a>
                            <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/random_forest_baverages.ipynb" download class="button">
                                <i class="fas fa-download"></i> Notebook
                            </a>
                            <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/Beverage.csv" download class="button">
                                <i class="fas fa-database"></i> Dataset
                            </a>
                            <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/Beverage%20Analysis%20Report.pdf" class="button">
                                <i class="fas fa-file-pdf"></i> Report
                            </a>
                        </div>
                    </div>
                    <div class="highlights">
                        <h3>Key Highlights</h3>
                        <ul>
                            <li><strong>Dataset:</strong> 4898 beverages with 11 physicochemical features</li>
                            <li><strong>Target:</strong> Binary classification (Excellent/Other)</li>
                            <li><strong>Techniques:</strong> Random Forest, Decision Tree, PCA, K-Means</li>
                            <li><strong>Best Accuracy:</strong> 0.881 with Random Forest</li>
                        </ul>
                    </div>
                    <div class="project-structure" style="margin-top:2em;">
                        <h3>Project Structure</h3>
<pre>random_forest_example/
│   .gitignore
│   Beverage Analysis Report.pdf
│   Beverage.csv
│   random_forest_baverages.ipynb
│   random_forest_baverages.py
│   README.md
│   requirements.txt
│
└───plots
    │   confusion_matrix.png
    │   elbow_method.png
    │   feature_importance.png
    │   k_means_clustering.png
    │   pca.png
</pre>
                    </div>
                </section>

                <!-- Analysis -->
                <section id="analysis" class="tab-content">
                    <h2>Beverage Quality Prediction with Random Forest</h2>
                    <p>
                        This notebook demonstrates a machine learning approach to predict beverage quality using Random Forest Classifier.
                    </p>
                    <h3>Setup</h3>
                    <pre><code class="language-python"># Install required packages
!pip install scikit-learn imbalanced-learn seaborn matplotlib numpy pandas

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier</code></pre>

                    <h3>Data Loading and Preprocessing</h3>
                    <pre><code class="language-python"># Download the dataset
!wget -O Beverage.csv https://raw.githubusercontent.com/tomekbiel/python_data_analysis/refs/heads/random_forest_example/Beverage.csv

# Load the dataset
dataset = pd.read_csv("Beverage.csv")

# Convert quality to binary
def converter(column):
        if column == 'Excellent':
          return 1
        else:
          return 0

dataset['quality'] = dataset['quality'].apply(converter)
print("Dataset Info:")
print(dataset.info())

# One-hot encode categorical features
categorical_features = ['alcohol', 'volatile acidity', 'sulphates']
f_dat = pd.get_dummies(dataset, columns=categorical_features)
print("\nAfter One-Hot Encoding:")
print(f_dat.head())</code></pre>
<pre class="output">--2025-06-25 10:48:58--  https://raw.githubusercontent.com/tomekbiel/python_data_analysis/refs/heads/random_forest_example/Beverage.csv
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 291534 (285K) [text/plain]
Saving to: ‘Beverage.csv’

Beverage.csv        100%[===================>] 284.70K  --.-KB/s    in 0.03s   

2025-06-25 10:48:58 (10.7 MB/s) - ‘Beverage.csv’ saved [291534/291534]

Dataset Info:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4898 entries, 0 to 4897
Data columns (total 12 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   fixed acidity         4898 non-null   float64
 1   volatile acidity      4898 non-null   float64
 2   citric acid           4898 non-null   float64
 3   residual sugar        4898 non-null   float64
 4   chlorides             4898 non-null   float64
 5   free sulfur dioxide   4898 non-null   float64
 6   total sulfur dioxide  4898 non-null   float64
 7   density               4898 non-null   float64
 8   pH                    4898 non-null   float64
 9   sulphates             4898 non-null   float64
 10  alcohol               4898 non-null   float64
 11  quality               4898 non-null   int64  
dtypes: float64(11), int64(1)
memory usage: 459.3 KB
None

After One-Hot Encoding:
   fixed acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  \
0            7.0         0.36            20.7      0.045                 45.0   
1            6.3         0.34             1.6      0.049                 14.0   
2            8.1         0.40             6.9      0.050                 30.0   
3            7.2         0.32             8.5      0.058                 47.0   
4            7.2         0.32             8.5      0.058                 47.0   

   total sulfur dioxide  density    pH  quality  alcohol_8.0  ...  \
0                 170.0   1.0010  3.00        0        False  ...   
1                 132.0   0.9940  3.30        0        False  ...   
2                  97.0   0.9951  3.26        0        False  ...   
3                 186.0   0.9956  3.19        0        False  ...   
4                 186.0   0.9956  3.19        0        False  ...   

   sulphates_0.94  sulphates_0.95  sulphates_0.96  sulphates_0.97  \
0           False           False           False           False   
1           False           False           False           False   
2           False           False           False           False   
3           False           False           False           False   
4           False           False           False           False   

   sulphates_0.98  sulphates_0.99  sulphates_1.0  sulphates_1.01  \
0           False           False          False           False   
1           False           False          False           False   
2           False           False          False           False   
3           False           False          False           False   
4           False           False          False           False   

   sulphates_1.02  sulphates_1.03  
0           False           False  
1           False           False  
2           False           False  
3           False           False  
4           False           False  

[5 rows x 316 columns]</pre>

                    <h3>Exploratory Data Analysis</h3>
                    <pre><code class="language-python"># Basic statistics
print("\nBasic Statistics:")
print(dataset.describe())

# Check class distribution
print("\nClass Distribution:")
print(dataset['quality'].value_counts())

# Correlation matrix
corr = dataset.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()</code></pre>
                    

<pre class="output">Basic Statistics:
       fixed acidity  volatile acidity  citric acid  residual sugar  \
count    4898.000000       4898.000000  4898.000000     4898.000000   
mean        6.854788          0.278241     0.334192        6.391415   
std         0.843868          0.100795     0.121020        5.072058   
min         3.800000          0.080000     0.000000        0.600000   
25%         6.300000          0.210000     0.270000        1.700000   
50%         6.800000          0.260000     0.320000        5.200000   
75%         7.300000          0.320000     0.390000        9.900000   
max        14.200000          1.100000     1.660000       65.800000   

         chlorides  free sulfur dioxide  total sulfur dioxide      density  \
count  4898.000000          4898.000000           4898.000000  4898.000000   
mean      0.045772            35.308085            138.360657     0.994027   
std       0.021848            17.007137             42.498065     0.002991   
min       0.009000             2.000000              9.000000     0.987110   
25%       0.036000            23.000000            108.000000     0.991723   
50%       0.043000            34.000000            134.000000     0.993740   
75%       0.050000            46.000000            167.000000     0.996100   
max       0.346000           289.000000            440.000000     1.038980   

                pH    sulphates      alcohol      quality  
count  4898.000000  4898.000000  4898.000000  4898.000000  
mean      3.188267     0.489847    10.514267     0.216415  
std       0.151001     0.114126     1.230621     0.411842  
min       2.720000     0.220000     8.000000     0.000000  
25%       3.090000     0.410000     9.500000     0.000000  
50%       3.180000     0.470000    10.400000     0.000000  
75%       3.280000     0.550000    11.400000     0.000000  
max       3.820000     1.080000    14.200000     1.000000  

Class Distribution:
quality
0    3838
1    1060
Name: count, dtype: int64</pre>
                    <div class="plot-container">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/plots/correlation_matrix.png" 
                             alt="Correlation Matrix" 
                             class="plot-image">
                        <p class="plot-caption">Correlation Matrix showing relationships between different features</p>
                    </div>

                    <h3>Data Preparation</h3>
                    <pre><code class="language-python"># Prepare features and target
X = f_dat.drop('quality', axis=1)
Y = f_dat['quality']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.3, random_state=100)

# Handle class imbalance with SMOTE
print("Before SMOTE:", pd.Series(Y_train).value_counts())
smote = SMOTE(random_state=101)
X_train, Y_train = smote.fit_resample(X_train, Y_train)
print("After SMOTE:", pd.Series(Y_train).value_counts())</code></pre>
<pre><code class="language-text">Before SMOTE: quality
0    2683
1     745
Name: count, dtype: int64
After SMOTE: quality
1    2683
0    2683
Name: count, dtype: int64</code></pre>

                    <h3>Model Training and Evaluation</h3>
                    <pre><code class="language-python"># Train and evaluate Random Forest
rfc = RandomForestClassifier(n_estimators=400, criterion='entropy', max_features='sqrt', random_state=1)
rfc.fit(X_train, Y_train)

# Make predictions
Y_pred = rfc.predict(X_test)

# Evaluate the model
accuracy = metrics.accuracy_score(Y_test, Y_pred)
conf_mat = metrics.confusion_matrix(Y_test, Y_pred)

print(f"Accuracy: {accuracy:.4f}")
print('\nConfusion Matrix:')
print(conf_mat)
print('\nClassification Report:')
print(metrics.classification_report(Y_test, Y_pred))</code></pre>
                    
                    <pre><code class="language-python"># Confusion Matrix Visualization
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Excellent', 'Excellent'], yticklabels=['Not Excellent', 'Excellent'])
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()</code></pre>
                    

<pre><code class="language-text">Accuracy: 0.8810

Confusion Matrix:
[[1086   69]
 [ 106  209]]

Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.94      0.93      1155
           1       0.75      0.66      0.70       315

    accuracy                           0.88      1470
   macro avg       0.83      0.80      0.82      1470
weighted avg       0.88      0.88      0.88      1470</code></pre>

                    <h3>Feature Importance</h3>
                    <pre><code class="language-python"># Get feature importances
importances = rfc.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(12, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()</code></pre>
                    
                    <div class="plot-container">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/plots/feature_importance.png" 
                             alt="Feature Importance" 
                             class="plot-image">
                        <p class="plot-caption">Feature Importance from Random Forest Model</p>
                    </div>
                    


                    <h3>Dimensionality Reduction with PCA</h3>
                    <pre><code class="language-python"># Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total explained variance ratio:", sum(pca.explained_variance_ratio_))

# Visualize the first two principal components
plt.figure(figsize=(10, 8))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=Y, cmap='viridis', alpha=0.7)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA - First Two Principal Components')
plt.colorbar(label='Quality (0=Not Excellent, 1=Excellent)')
plt.show()</code></pre>
                    
                    <div class="plot-container">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/plots/pca.png" 
                             alt="PCA Visualization" 
                             class="plot-image">
                        <p class="plot-caption">PCA - First Two Principal Components</p>
                    </div>
                    

<pre><code class="language-text">Explained variance ratio: [0.01092321 0.00802697]
Total explained variance ratio: 0.018950179766635992</code></pre>
                    
                    <div class="plot-container">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/plots/pca.png" 
                             alt="PCA Visualization" 
                             class="plot-image">
                        <p class="plot-caption">PCA - First Two Principal Components</p>
                    </div>

                    <h3>Clustering with K-Means</h3>
                    
                    <pre><code class="language-python"># Determine optimal number of clusters using Elbow Method
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=100)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot Elbow curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.xticks(range(1, 11))
plt.grid(True)
plt.show()

# Apply K-Means with optimal k (e.g., 4)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=100)
cluster_labels = kmeans.fit_predict(X_scaled)

# Visualize clusters using PCA components
plt.figure(figsize=(10, 8))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', marker='X', label='Centroids')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title(f'K-Means Clustering (k={optimal_k}) on PCA Components')
plt.legend()
plt.colorbar(label='Cluster Label')
plt.show()</code></pre>
                    
                    <div class="plot-container">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/plots/elbow_method.png" 
                             alt="Elbow Method" 
                             class="plot-image">
                        <p class="plot-caption">Elbow Method for determining optimal number of clusters</p>
                    </div>
                    
                    <div class="plot-container">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/plots/k_means_clustering.png" 
                             alt="K-Means Clustering" 
                             class="plot-image">
                        <p class="plot-caption">K-Means Clustering (k=4) on PCA Components</p>
                    </div>

                    <h3 style="margin-top:2em;">Jupyter Notebook</h3>
                    <p>The complete analysis is available in the interactive Jupyter Notebook containing all code, visualizations, and explanations:</p>
                    <div class="notebook-preview" style="margin-bottom:1.5em;">
                        <img src="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/wyniki/notebook_preview.png" 
                             alt="Notebook Preview" 
                             style="max-width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"
                             onerror="this.src='https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/wyniki/feature_importance.png'">
                                        </div>
                    <div class="notebook-actions" style="margin-bottom:1.5em; display: flex; flex-wrap: wrap; gap: 12px;">
                        <a href="https://colab.research.google.com/github/tomekbiel/python_data_analysis/blob/main/random_forest_example/random_forest_baverages.ipynb" 
                        target="_blank" 
                        class="button primary"
                        title="Open in Google Colab">
                            <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab" style="height:20px;">
                        </a>
                        <a href="https://nbviewer.org/github/tomekbiel/python_data_analysis/blob/random_forest_example/random_forest_baverages.ipynb" 
                        target="_blank" 
                        class="button"
                        title="View in NBViewer">
                            <i class="fas fa-eye" style="font-size:20px;"></i>
                        </a>
                        <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/main/random_forest_example/random_forest_baverages.ipynb" 
                        download 
                        class="button"
                        title="Download Notebook">
                            <i class="fas fa-download" style="font-size:20px;"></i>
                        </a>
                    </div>
                    <div class="notebook-content">
                        <h4>Notebook Highlights</h4>
                        <ul>
                            <li>Complete step-by-step analysis</li>
                            <li>Interactive code cells</li>
                            <li>Detailed explanations of each step</li>
                            <li>All visualizations with interpretations</li>
                        </ul>
                    </div>
                </section>

                <!-- Technical Report -->
                <section id="report" class="tab-content">
                    <h2>Technical Report</h2>
                    <!-- 1. Introduction -->
                    <section>
                        <h3>1. Introduction</h3>
                        <p>This study presents a classification analysis of beverage quality using supervised and unsupervised machine learning techniques. The data includes 4898 observations with 11 physicochemical numerical features and one categorical target variable: quality. The aim is to build a predictive model capable of distinguishing high-quality beverages (labelled "Excellent") from others.</p>
                    </section>
                    <!-- 2. Data Preparation -->
                    <section>
                        <h3>2. Data Preparation and Preprocessing</h3>
                        <ul>
                            <li><strong>Target Transformation:</strong> The quality variable was converted to binary:
                                <ul>
                                    <li>Excellent → 1</li>
                                    <li>All other values → 0</li>
                                </ul>
                            </li>
                            <li><strong>Feature Engineering:</strong> Key variables (alcohol, volatile acidity, sulphates) were one-hot encoded</li>
                            <li><strong>Scaling:</strong> All features were standardized using StandardScaler</li>
                            <li><strong>Class Imbalance:</strong> SMOTE was applied to balance the dataset (original: 745 vs. 2683, after: 2683 per class)</li>
                        </ul>
                    </section>
                    <!-- 3. Supervised Modeling -->
                    <section>
                        <h3>3. Supervised Modeling</h3>
                        <h4>3.1 Decision Tree Classifier</h4>
                        <ul>
                            <li>Criterion: Entropy</li>
                            <li>Max Depth: 5</li>
                            <li>Accuracy: 0.715</li>
                            <li>Confusion Matrix:
                                <ul>
                                    <li>TP: 215</li>
                                    <li>TN: 836</li>
                                    <li>FP: 319</li>
                                    <li>FN: 100</li>
                                </ul>
                            </li>
                            <li>Cross-validation Accuracy: Mean = 0.724 (± 0.022)</li>
                        </ul>
                        <h4>3.2 Random Forest Classifier</h4>
                        <ul>
                            <li>Initial Model: n_estimators=300, max_features='sqrt', criterion='entropy'</li>
                            <li>Accuracy: 0.883</li>
                            <li>Grid Search Optimization:
                                <ul>
                                    <li>Best parameters: n_estimators=450, max_depth=30</li>
                                    <li>Cross-validated score: 0.918</li>
                                </ul>
                            </li>
                            <li>Final accuracy on test data: 0.881</li>
                            <li>Feature Importance:
                                <ul>
                                    <li>density: 0.401</li>
                                    <li>residual sugar: 0.198</li>
                                    <li>citric acid: 0.187</li>
                                    <li>free sulfur dioxide: 0.093</li>
                                </ul>
                            </li>
                        </ul>
                    </section>
                    <!-- 4. Unsupervised Learning -->
                    <section>
                        <h3>4. Unsupervised Learning: PCA and Clustering</h3>
                        <h4>4.1 Principal Component Analysis (PCA)</h4>
                        <ul>
                            <li>Two components were extracted</li>
                            <li>Explained Variance Ratio: 1.89% total (Component 1: 1.09%, Component 2: 0.80%)</li>
                            <li>PCA visualization showed clustering trends with several outliers</li>
                        </ul>
                        <h4>4.2 KMeans Clustering</h4>
                        <ul>
                            <li>Elbow method identified 4 as the optimal number of clusters</li>
                            <li>Clustering helped visualize sample groupings</li>
                        </ul>
                    </section>
                    <!-- 5. Recommendations -->
                    <section>
                        <h3>5. Recommendations</h3>
                        <p>Based on the models and their interpretation:</p>
                        <ul>
                            <li>High-quality beverages tend to have:
                                <ul>
                                    <li>Alcohol content: optimal at ~9.2-9.4</li>
                                    <li>Low volatile acidity, residual sugar, and sulphates</li>
                                </ul>
                            </li>
                            <li>The most predictive variable is density, followed by indicators of sugar and acidity</li>
                            <li>Feature importance analysis suggests that after one-hot encoding, numerical distributions lose some predictive power</li>
                        </ul>
                    </section>
                    <!-- 6. Conclusion -->
                    <section>
                        <h3>6. Conclusion</h3>
                        <p>This project demonstrates the effective use of classification models and dimensionality reduction to predict product quality from chemical properties. Random Forest proved to be the most robust approach, particularly after parameter tuning and class balancing. PCA and KMeans provided valuable support for feature insight and unsupervised validation.</p>
                        <p>The methods used can be applied to other industrial datasets for quality control, especially where domain knowledge complements machine learning.</p>
                    </section>
                    <!-- Resources Section -->
                    <div class="resources-section" style="margin-top:2em;">
                        <h3>Resources</h3>
                        <div class="resource-buttons">
                            <a href="https://github.com/tomekbiel/python_data_analysis/random_forest_example/random_forest_baverages.ipynb" target="_blank" class="button">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                            <a href="https://colab.research.google.com/github/tomekbiel/python_data_analysis/random_forest_example/random_forest_baverages.ipynb" target="_blank" class="button">
                                <i class="fab fa-google"></i> Colab
                            </a>
                            <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/random_forest_baverages.ipynb" download class="button">
                                <i class="fas fa-download"></i> Notebook
                            </a>
                            <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/Beverage.csv" download class="button">
                                <i class="fas fa-database"></i> Dataset
                            </a>
                            <a href="https://raw.githubusercontent.com/tomekbiel/python_data_analysis/random_forest_example/Beverage%20Analysis%20Report.pdf" class="button">
                                <i class="fas fa-file-pdf"></i> Report
                            </a>
                        </div>
                    </div>
                </section>
            </main>
        </div>
        <footer class="footer full-width">
            <div class="footer-bottom">
                &copy; 2025 AlfaKernel | All rights reserved | Author: Tomasz Biel
            </div>
        </footer>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="../../../js/code-highlight.js"></script>
    <script src="../../../js/script.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            const tabs = document.querySelectorAll('.sidebar .tab');
            const sections = document.querySelectorAll('.tab-content');
            
            tabs.forEach(tab => {
                tab.addEventListener('click', function() {
                    tabs.forEach(t => t.classList.remove('active'));
                    sections.forEach(s => s.classList.remove('active'));
                    this.classList.add('active');
                    document.getElementById(this.getAttribute('data-tab')).classList.add('active');
                    window.scrollTo({top: 0, behavior: 'smooth'});
                });
            });
        });
    </script>
</body>
</html>